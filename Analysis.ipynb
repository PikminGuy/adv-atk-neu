{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import ParticleClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shawn/miniconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Data loaded into self.images, self.labels\n",
      "Split into self.images_train, self.images_test, self.labels_train, self.labels_test\n",
      "Train and test labels encoded into self.y_train and self.y_test\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "WARNING:tensorflow:From /Users/shawn/miniconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1920/1920 [==============================] - 7s 3ms/sample - loss: 0.5348 - acc: 0.7620 - val_loss: 0.3687 - val_acc: 0.8083\n",
      "Epoch 2/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.2230 - acc: 0.9167 - val_loss: 0.1361 - val_acc: 0.9479\n",
      "Epoch 3/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1853 - acc: 0.9359 - val_loss: 0.1352 - val_acc: 0.9458\n",
      "Epoch 4/10\n",
      "1920/1920 [==============================] - 6s 3ms/sample - loss: 0.1371 - acc: 0.9521 - val_loss: 0.1642 - val_acc: 0.9438\n",
      "Epoch 5/10\n",
      "1920/1920 [==============================] - 6s 3ms/sample - loss: 0.1261 - acc: 0.9578 - val_loss: 0.1303 - val_acc: 0.9479\n",
      "Epoch 6/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1323 - acc: 0.9547 - val_loss: 0.1579 - val_acc: 0.9312\n",
      "Model trained.\n",
      "600/600 [==============================] - 0s 667us/sample - loss: 0.1637 - acc: 0.9383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       187\n",
      "           1       0.91      0.98      0.94       204\n",
      "           2       0.93      0.89      0.91       209\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       600\n",
      "   macro avg       0.94      0.94      0.94       600\n",
      "weighted avg       0.94      0.94      0.94       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = ParticleClassifier()\\\n",
    "                .load_data('data/toy_data.csv')\\\n",
    "                .train_test_split()\\\n",
    "                .pre_proc_images(train=True, test=True)\\\n",
    "                .one_hot_encode_labels(train=True, test=True)\\\n",
    "                .train_model()\\\n",
    "                .evaluate_model()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test labels encoded into self.y_train and self.y_test\n",
      "Labels: [187 204 209] \n",
      " Original Predictions: [181 219 200] \n",
      " Attacked Predictions: [178 220 202]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       181\n",
      "           1       0.99      1.00      0.99       219\n",
      "           2       0.99      1.00      1.00       200\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       600\n",
      "   macro avg       0.99      0.99      0.99       600\n",
      "weighted avg       0.99      0.99      0.99       600\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<model.ParticleClassifier at 0x1a2f413080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hot pixel attack\n",
    "classifier.hot_pixel_attack()\\\n",
    "          .pre_proc_images(attacked=True)\\\n",
    "          .one_hot_encode_labels(attacked=True)\\\n",
    "          .evaluate_attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into self.images, self.labels\n",
      "Split into self.images_train, self.images_test, self.labels_train, self.labels_test\n",
      "Train and test labels encoded into self.y_train and self.y_test\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/10\n",
      "1920/1920 [==============================] - 6s 3ms/sample - loss: 0.4998 - acc: 0.7896 - val_loss: 0.2661 - val_acc: 0.8833\n",
      "Epoch 2/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1917 - acc: 0.9302 - val_loss: 0.1658 - val_acc: 0.9354\n",
      "Epoch 3/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1723 - acc: 0.9365 - val_loss: 0.2053 - val_acc: 0.9250\n",
      "Epoch 4/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1204 - acc: 0.9552 - val_loss: 0.1287 - val_acc: 0.9542\n",
      "Epoch 5/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.1167 - acc: 0.9604 - val_loss: 0.1461 - val_acc: 0.9417\n",
      "Epoch 6/10\n",
      "1920/1920 [==============================] - 5s 3ms/sample - loss: 0.0985 - acc: 0.9661 - val_loss: 0.1467 - val_acc: 0.9521\n",
      "Model trained.\n",
      "600/600 [==============================] - 0s 689us/sample - loss: 0.1472 - acc: 0.9583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       208\n",
      "           1       0.96      0.97      0.97       200\n",
      "           2       0.94      0.93      0.93       192\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       600\n",
      "   macro avg       0.96      0.96      0.96       600\n",
      "weighted avg       0.96      0.96      0.96       600\n",
      "\n",
      "Train and test labels encoded into self.y_train and self.y_test\n",
      "Labels: [208 200 192] \n",
      " Original Predictions: [207 204 189] \n",
      " Attacked Predictions: [ 15 185 400]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.14       207\n",
      "           1       0.96      0.87      0.91       204\n",
      "           2       0.46      0.97      0.62       189\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       600\n",
      "   macro avg       0.81      0.64      0.56       600\n",
      "weighted avg       0.82      0.63      0.55       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Background Neutrons (2x2) square of relatively high value\n",
    "value =  np.mean([classifier.images.max(), classifier.images.mean()])\n",
    "classifier = ParticleClassifier()\\\n",
    "                .load_data('data/toy_data.csv')\\\n",
    "                .train_test_split()\\\n",
    "                .pre_proc_images(train=True, test=True, filters=False)\\\n",
    "                .one_hot_encode_labels(train=True, test=True)\\\n",
    "                .train_model()\\\n",
    "                .evaluate_model()\\\n",
    "                .apply_attack(classifier.add_hot_area, size=(2,2), value=value)\\\n",
    "               .pre_proc_images(attacked=True, filters=False)\\\n",
    "               .one_hot_encode_labels(attacked=True)\\\n",
    "               .evaluate_attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = classfier.images\n",
    "value =  np.mean([classifier.images.max(), classifier.images.mean()])\n",
    "classifier.apply_attack(classifier.add_hot_area, size=(2,2), value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([classifier.images.max(), classifier.images.mean()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
